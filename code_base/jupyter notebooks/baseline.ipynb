{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE542FinalProject_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFI3RifcH2M1",
        "colab_type": "code",
        "outputId": "c604d66e-6f0e-4905-b775-1064e707915c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import SGD, Adam\n",
        "import tensorflow as tf\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "026Cwq6fH9Gj",
        "colab_type": "code",
        "outputId": "935b7fef-0623-4a3f-96f3-3f9eb52e65b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McAKIcQCH-7t",
        "colab_type": "code",
        "outputId": "beb3c042-957d-454c-b381-476922e30261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = \"gdrive/My Drive/NeuralNetCapstone/\"\n",
        "picklefile = \"onechannel.pkl\"\n",
        "\n",
        "with open(os.path.join(path,picklefile), \"rb\") as f:\n",
        "    images = pickle.load(f)\n",
        "\n",
        "print (\"Shape:\",images.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (51759, 128, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4tSolwWICcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get y labels from data file\n",
        "datasheetfile = \"final_df.csv\"\n",
        "df = pd.read_csv(os.path.join(path,datasheetfile))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjt1cHLZIEKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_classes = df.columns.values[-14:]\n",
        "y_true = df[all_classes].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftIuNHLin8J_",
        "colab_type": "code",
        "outputId": "08536594-3e76-4f11-a722-20de212b2148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y_true.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51759, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHolJFOZsdlM",
        "colab_type": "code",
        "outputId": "a05c12c2-ec70-48ed-e04c-939075e523b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def get_extra_images(disease):\n",
        "    path = \"gdrive/My Drive/NeuralNetCapstone/\"\n",
        "    picklefile = None\n",
        "    if disease == 0:\n",
        "        picklefile = \"gan_alec500.pkl\"\n",
        "    if disease == 4:\n",
        "        picklefile = \"gan_effusion.pkl\"\n",
        "    if disease == 7:\n",
        "        picklefile = \"hernia_aug.pkl\"\n",
        "    elif disease == 8:\n",
        "        picklefile = \"gan_infil.pkl\"\n",
        "    elif disease == 10:\n",
        "        picklefile = \"gan_nodule.pkl\"\n",
        "    elif disease == 12:\n",
        "        picklefile = \"pneumonia_aug.pkl\"\n",
        "    \n",
        "\n",
        "    resized_images = []\n",
        "    with open(os.path.join(path,picklefile), \"rb\") as f:\n",
        "        new_images = pickle.load(f)\n",
        "        print(\"Initial Shape: \" + str(new_images.shape))\n",
        "\n",
        "        if disease == 8 or disease == 0 or disease == 4 or disease == 10:\n",
        "            for i in range(len(new_images)):\n",
        "                im = (new_images[i]*255).astype('uint8').reshape([64, 64])\n",
        "                #print(im.shape)\n",
        "                im2 = Image.fromarray(im)\n",
        "                im3 = im2.resize((128, 128), Image.BICUBIC)\n",
        "                im4 = np.array(im3)\n",
        "                resized_images.append(im4)\n",
        "            new_images = resized_images\n",
        "            del resized_images\n",
        "            gc.collect()\n",
        "    \n",
        "    truth_array = []\n",
        "    for i in range(len(new_images)):\n",
        "        truth_vals = [0]*14\n",
        "        truth_vals[disease] = 1\n",
        "        truth_array.append(truth_vals)\n",
        "    truth_array = np.array(truth_array)\n",
        "    \n",
        "    #print(\"New Images Shape: \" + str(new_images.shape))\n",
        "    #print(\"Ground Truth Shape: \" + str(truth_array.shape))\n",
        "    #print(truth_array[0:10])\n",
        "    \n",
        "    if disease == 8 or disease == 0 or disease == 4 or disease == 10:\n",
        "        new_images = np.array(new_images).reshape([-1, 128, 128, 1])\n",
        "    \n",
        "    return np.array(new_images[0:3000]), np.array(truth_array[0:3000])\n",
        "  \n",
        "aug_x, aug_y = get_extra_images(10)\n",
        "print(\"Augmented X Shape: \" + str(aug_x.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Shape: (7386, 64, 64, 1)\n",
            "Augmented X Shape: (3000, 128, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwYgWDde5C0g",
        "colab_type": "code",
        "outputId": "518b5bec-40ed-4f40-cf93-34dde4c57540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#aug_x.reshape([-1, 128, 128, 1])\n",
        "aug_x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 128, 128, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdCl19qYIwys",
        "colab_type": "code",
        "outputId": "f9424c49-d286-48fe-fa86-621da942d289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(images, y_true, test_size=0.2, random_state=0)\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=0)\n",
        "\n",
        "del images\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHO9Zl344aB-",
        "colab_type": "code",
        "outputId": "d4540fa5-a25a-4d83-c285-a3e885c3f24f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "train_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33125, 128, 128, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMqckRBeI3Kq",
        "colab_type": "code",
        "outputId": "673c70dd-f73d-4d93-97e7-0f1999576f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_X = np.vstack((train_X, aug_x))\n",
        "train_y = np.vstack((train_y, aug_y))\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(36125, 128, 128, 1)\n",
            "(36125, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boDmiGavJYg7",
        "colab_type": "code",
        "outputId": "3c35ada1-4db5-47c3-e4d0-8d7ccb9154cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del aug_x\n",
        "del aug_y\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6CfxJe-Jis2",
        "colab_type": "code",
        "outputId": "94670ef5-7224-47da-98b3-181d174c9c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "def get_binary_truth(y):\n",
        "    binary_y = []\n",
        "    for i in range(14):\n",
        "        class_y = []\n",
        "        for j in range(len(y)):\n",
        "            val = int(y[j][i])\n",
        "            one_hot = np.array([0, 0])\n",
        "            one_hot[val] = 1\n",
        "            class_y.append(one_hot)\n",
        "        class_y = np.array(class_y)\n",
        "        class_y = np.reshape(class_y, (-1, 2))\n",
        "        binary_y.append(np.array(class_y))\n",
        "    return binary_y\n",
        "\n",
        "train_binary_y = get_binary_truth(train_y)\n",
        "val_binary_y = get_binary_truth(val_y)\n",
        "test_binary_y = get_binary_truth(test_y)\n",
        "\n",
        "print(np.shape(train_binary_y))\n",
        "print(train_binary_y[13])\n",
        "print(train_binary_y[13].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 36125, 2)\n",
            "[[1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " ...\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]]\n",
            "(36125, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIRzWw6bJI-q",
        "colab_type": "text"
      },
      "source": [
        "#Functions to create CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bOKdU_zI-vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act_functions = {'relu':tf.nn.relu, 'sigmoid':tf.nn.sigmoid, 'tanh':tf.nn.tanh}\n",
        "\n",
        "def conv2d(x, W, b, function, strides=1):\n",
        "    \n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    \n",
        "    func = act_functions[function]\n",
        "    \n",
        "    return func(x)\n",
        "    \n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
        "\n",
        "    \n",
        "def getWeightsAndBiases(size, filters, outputs, pooling, channels, neural_neurons, classes, beta):\n",
        "    weights = {}\n",
        "    biases = {}\n",
        "    regularizer = tf.contrib.layers.l2_regularizer(scale=beta)\n",
        "    \n",
        "    #Generate weights and biases for convolutional layers\n",
        "    for i in range(len(outputs)):\n",
        "        filter_size = filters[i]\n",
        "        #Size of inputs to layer is the same as outputs of previous layer\n",
        "        inputs = channels\n",
        "        if i > 0:\n",
        "            inputs = outputs[i-1]\n",
        "    \n",
        "        #Create weights and biases dictionary\n",
        "        weight_shape = (filter_size, filter_size, inputs, outputs[i])\n",
        "        bias_shape = (outputs[i])\n",
        "        \n",
        "        #Update size of images in layers based on pooling filter\n",
        "        size = int(np.ceil(size/pooling[i]))\n",
        "        \n",
        "        weights[str(i)] = tf.get_variable(\"W\" + str(i), \n",
        "                                          shape = weight_shape, \n",
        "                                          initializer = tf.contrib.layers.xavier_initializer(),\n",
        "                                          regularizer = regularizer)\n",
        "        biases[str(i)] = tf.get_variable(\"B\" + str(i), \n",
        "                                         shape = (bias_shape), \n",
        "                                         initializer = tf.contrib.layers.xavier_initializer())\n",
        "        \n",
        "    #Generate weights and biases for neural network layers\n",
        "    weights[str(i+1)] = tf.get_variable(\"W\" + str(i+1), \n",
        "                                        shape = (size*size*outputs[i], neural_neurons), \n",
        "                                        initializer = tf.contrib.layers.xavier_initializer(),\n",
        "                                        regularizer = regularizer)\n",
        "    biases[str(i+1)] = tf.get_variable(\"B\" + str(i+1),\n",
        "                                        shape = (neural_neurons),\n",
        "                                        initializer = tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    #Generate weights and biases for output layer\n",
        "    weights[str(i+2)] = tf.get_variable(\"W\" + str(i+2),\n",
        "                                        shape = (neural_neurons, classes),\n",
        "                                        initializer = tf.contrib.layers.xavier_initializer(),\n",
        "                                        regularizer = regularizer)\n",
        "    biases[str(i+2)] = tf.get_variable(\"B\" + str(i+2),\n",
        "                                        shape = (classes),\n",
        "                                        initializer = tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    return weights, biases\n",
        "    \n",
        "def createCNN(x, weights, biases, pooling, dropout, function):\n",
        "    \n",
        "    prev_layer = x\n",
        "    print(\"Input Layer Shape: \" + str(prev_layer.shape))\n",
        "    for i in range(len(pooling)):\n",
        "        #Create layer\n",
        "        layer = conv2d(prev_layer, weights[str(i)], biases[str(i)], function)\n",
        "        #Downsample layer\n",
        "        layer = maxpool2d(layer, k=pooling[i])\n",
        "        \n",
        "        prev_layer= layer\n",
        "    \n",
        "    #Add dropout layer\n",
        "    drop1_layer = tf.layers.dropout(inputs = prev_layer, rate=dropout[0])\n",
        "    \n",
        "    #Flatten CNN\n",
        "    fc1 = tf.reshape(drop1_layer, [-1, weights[str(i+1)].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights[str(i+1)]), biases[str(i+1)])\n",
        "    func = act_functions[function]\n",
        "    fc1 = func(fc1)\n",
        "    \n",
        "    #Add dropout layer\n",
        "    dropout_layer = tf.layers.dropout(inputs = fc1, rate=dropout[1])\n",
        "    \n",
        "    #Perform final layer operations and output appropriate number of outputs\n",
        "    out = tf.add(tf.matmul(dropout_layer, weights[str(i+2)]), biases[str(i+2)])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1e4o8DSJLhV",
        "colab_type": "text"
      },
      "source": [
        "#Create and Train CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPCBtPgvJNcK",
        "colab_type": "code",
        "outputId": "101a4815-9089-4de0-f207-a1401e29ee04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8538
        }
      },
      "source": [
        "#Reset TensorFlow\n",
        "tf.reset_default_graph()\n",
        "preds = []\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "#Create data placeholders for inputs and input labels (None value is batch size)\n",
        "size = train_X.shape[1]\n",
        "classes = 2\n",
        "channels = train_X.shape[3]\n",
        "x = tf.placeholder('float', [None, size, size, channels])\n",
        "y = tf.placeholder('float', [None, classes])\n",
        "\n",
        "#Hyperparameters\n",
        "outputs = [32, 64]\n",
        "learn_rate = 0.000001\n",
        "dropout_rate = [0.1, 0.5]\n",
        "batch_size = 256\n",
        "neural_neurons = 1024\n",
        "curr_class = 10\n",
        "beta = 0.05\n",
        "\n",
        "filters = [3, 4]\n",
        "pooling = [2, 2]\n",
        "func_name = 'relu'\n",
        "weights, biases = getWeightsAndBiases(size, filters, outputs, pooling, channels, neural_neurons, classes, beta)\n",
        "predictor = createCNN(x, weights, biases, pooling, dropout_rate, func_name)\n",
        "\n",
        "#Calculate weight for cost function\n",
        "weight_ratio = 0 \n",
        "current = train_binary_y[curr_class]\n",
        "for j in range(len(current)):\n",
        "  weight_ratio += current[j][1]\n",
        "weight_ratio /= len(current)\n",
        "print(\"Weight Ratio: \" + str(weight_ratio))\n",
        "\n",
        "#Populate and train CNN\n",
        "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictor, labels=y))\n",
        "cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=predictor, targets=y, pos_weight = 1/0.122))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate= learn_rate).minimize(cost)\n",
        "\n",
        "#Create nodes to assess network accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(predictor, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "#Initialize all variables (including weights and biases)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run CNN\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    train_accuracy = []\n",
        "    val_accuracy = []\n",
        "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
        "\n",
        "    epochs = 30\n",
        "    batch_size = batch_size\n",
        "\n",
        "    key = str(outputs)+\"_\"+str(learn_rate)+\"_\"+str(dropout_rate)+\"_\"+str(batch_size)\n",
        "    print(key)\n",
        "    \n",
        "    train_indices = range(0, len(train_X))\n",
        "    train_indices = shuffle(train_indices)\n",
        "\n",
        "    val_indices = range(0, len(val_X))\n",
        "    val_indices = shuffle(val_indices)\n",
        "    \n",
        "    test_indices = range(0, len(test_X))\n",
        "    test_indices = shuffle(test_indices)\n",
        "    \n",
        "    #Iterate for certain number of epochs\n",
        "    for i in range(epochs):\n",
        "        #Iterate through all batches in single epoch\n",
        "        for batch in range(len(train_X)//batch_size):\n",
        "            batch_x = generate_batch(train_X, batch, batch_size, train_indices)\n",
        "            batch_y = generate_batch(train_binary_y[curr_class], batch, batch_size, train_indices)\n",
        "            #batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
        "            #batch_y = train_binary_y[curr_class][batch*batch_size:min((batch+1)*batch_size,len(train_binary_y[curr_class]))]\n",
        "\n",
        "            #Run optimizer\n",
        "            opt = sess.run(optimizer, feed_dict = {x: batch_x, y: batch_y})\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict = {x: batch_x, y: batch_y})\n",
        "\n",
        "        print(\"Epoch \" + str(i) + \", Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
        "\n",
        "        #Calculate validation set statistics\n",
        "        tps = fps = tns = fns = 0\n",
        "        valid_accs = []\n",
        "        valid_losses = []\n",
        "        valid_aucs = []\n",
        "        val_batch_size = 2048\n",
        "        for val_batch in range(len(val_X)//val_batch_size):\n",
        "            val_batch_x = generate_batch(val_X, val_batch, val_batch_size, val_indices)\n",
        "            val_batch_y = generate_batch(val_binary_y[curr_class], val_batch, val_batch_size, val_indices)\n",
        "            #val_batch_x = val_X[val_batch*val_batch_size:min((val_batch+1)*val_batch_size,len(val_X))]\n",
        "            #val_batch_y = val_binary_y[curr_class][val_batch*val_batch_size:min((val_batch+1)*val_batch_size,len(val_binary_y[curr_class]))]\n",
        "            preds = predictor.eval(feed_dict = {x: val_batch_x})\n",
        "            valid_acc, valid_loss = sess.run([accuracy, cost], feed_dict = {x: val_batch_x, y: val_batch_y})\n",
        "            valid_accs.append(valid_acc)\n",
        "            valid_losses.append(valid_loss)\n",
        "            tp, fp, tn, fn, auc = conf_matrix(preds, val_batch_y)\n",
        "            tps += tp\n",
        "            fps += fp\n",
        "            tns += tn\n",
        "            fns += fn\n",
        "            valid_aucs.append(auc)\n",
        "        acc = (tps+tns)/(tps+fps+tns+fns)\n",
        "        recall = tps/(tps+fns)\n",
        "        precision = 0\n",
        "        if (tps+fps) > 0:\n",
        "            precision = tps/(tps+fps)\n",
        "        f1 = 2*tps/(2*tps+fps+fns)\n",
        "\n",
        "        print(\"Validation Accuracy:\",\"{:.5f}\".format(np.mean(valid_accs)))\n",
        "        print(\"Validation Loss:\",\"{:.5f}\".format(np.mean(valid_losses)))\n",
        "        print(\"Accuracy: \" + str(acc))\n",
        "        print(\"Recall: \" + str(recall))\n",
        "        print(\"Precision: \" + str(precision))\n",
        "        print(\"F1: \" + str(f1))\n",
        "        print(\"AUC: \" + str(np.mean(valid_aucs)))\n",
        "        print(\"\")\n",
        "        \n",
        "        #Calculate testing set statistics\n",
        "        tps = fps = tns = fns = 0\n",
        "        test_accs = []\n",
        "        test_losses = []\n",
        "        test_aucs = []\n",
        "        test_batch_size = val_batch_size\n",
        "        for test_batch in range(len(test_X)//test_batch_size):\n",
        "            test_batch_x = generate_batch(test_X, test_batch, test_batch_size, test_indices)\n",
        "            test_batch_y = generate_batch(test_binary_y[curr_class], test_batch, test_batch_size, test_indices)\n",
        "            #test_batch_x = test_X[test_batch*test_batch_size:min((test_batch+1)*test_batch_size,len(test_X))]\n",
        "            #test_batch_y = test_binary_y[curr_class][test_batch*test_batch_size:min((test_batch+1)*test_batch_size,len(test_binary_y[curr_class]))]\n",
        "            preds = predictor.eval(feed_dict = {x: test_batch_x})\n",
        "            test_acc, test_loss = sess.run([accuracy, cost], feed_dict = {x: test_batch_x, y: test_batch_y})\n",
        "            test_accs.append(test_acc)\n",
        "            test_losses.append(test_loss)\n",
        "            tp, fp, tn, fn, auc = conf_matrix(preds, test_batch_y)\n",
        "            tps += tp\n",
        "            fps += fp\n",
        "            tns += tn\n",
        "            fns += fn\n",
        "            test_aucs.append(auc)\n",
        "        acc = (tps+tns)/(tps+fps+tns+fns)\n",
        "        recall = tps/(tps+fns)\n",
        "        precision = 0\n",
        "        if (tps+fps) > 0:\n",
        "            precision = tps/(tps+fps)\n",
        "        f1 = 2*tps/(2*tps+fps+fns)\n",
        "\n",
        "        print(\"Testing Accuracy:\",\"{:.5f}\".format(np.mean(test_accs)))\n",
        "        print(\"Testing Loss:\",\"{:.5f}\".format(np.mean(test_losses)))\n",
        "        print(\"Accuracy: \" + str(acc))\n",
        "        print(\"Recall: \" + str(recall))\n",
        "        print(\"Precision: \" + str(precision))\n",
        "        print(\"F1: \" + str(f1))\n",
        "        print(\"AUC: \" + str(np.mean(test_aucs)))\n",
        "        print(\"\")\n",
        "        \n",
        "        '''valid_size = 2000\n",
        "        preds = predictor.eval(feed_dict = {x: val_X[0:valid_size]})\n",
        "        valid_acc, valid_loss = sess.run([accuracy, cost], feed_dict = {x: val_X[0:valid_size], y: val_binary_y[curr_class][0:valid_size]})\n",
        "        train_loss.append(loss)\n",
        "        val_loss.append(valid_loss)\n",
        "        train_accuracy.append(acc)\n",
        "        val_accuracy.append(valid_acc)\n",
        "        print(\"Validation Accuracy:\",\"{:.5f}\".format(valid_acc))\n",
        "        print(\"Validation Loss:\",\"{:.5f}\".format(valid_loss))\n",
        "        conf_matrix(preds, val_binary_y[curr_class], print_stats = True)'''\n",
        "\n",
        "        '''#Calculate accuracy for test set\n",
        "        test_acc, test_loss = sess.run([accuracy, cost], feed_dict = {x: test_X, y: test_y})\n",
        "        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
        "        print(\"Testing Loss:\",\"{:.5f}\".format(test_loss))'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Layer Shape: (?, 128, 128, 1)\n",
            "Weight Ratio: 0.19587543252595155\n",
            "[32, 64]_1e-06_[0.1, 0.5]_256\n",
            "Epoch 0, Loss= 1.504340, Training Accuracy= 0.82031\n",
            "Validation Accuracy: 0.82776\n",
            "Validation Loss: 1.51983\n",
            "Accuracy: 0.8277587890625\n",
            "Recall: 0.075\n",
            "Precision: 0.13368983957219252\n",
            "F1: 0.09609224855861627\n",
            "AUC: 0.5218929970323418\n",
            "\n",
            "Testing Accuracy: 0.83643\n",
            "Testing Loss: 1.41674\n",
            "Accuracy: 0.83642578125\n",
            "Recall: 0.08799342105263158\n",
            "Precision: 0.1589895988112927\n",
            "F1: 0.11328745367919534\n",
            "AUC: 0.543435785558849\n",
            "\n",
            "Epoch 1, Loss= 1.340096, Training Accuracy= 0.82422\n",
            "Validation Accuracy: 0.84241\n",
            "Validation Loss: 1.36008\n",
            "Accuracy: 0.8424072265625\n",
            "Recall: 0.062\n",
            "Precision: 0.1493975903614458\n",
            "F1: 0.08763250883392226\n",
            "AUC: 0.5318641249962625\n",
            "\n",
            "Testing Accuracy: 0.84922\n",
            "Testing Loss: 1.27439\n",
            "Accuracy: 0.84921875\n",
            "Recall: 0.0649671052631579\n",
            "Precision: 0.16255144032921812\n",
            "F1: 0.09283196239717979\n",
            "AUC: 0.5503546098159726\n",
            "\n",
            "Epoch 2, Loss= 1.339429, Training Accuracy= 0.85156\n",
            "Validation Accuracy: 0.86560\n",
            "Validation Loss: 1.30102\n",
            "Accuracy: 0.8656005859375\n",
            "Recall: 0.025\n",
            "Precision: 0.16556291390728478\n",
            "F1: 0.043440486533449174\n",
            "AUC: 0.5386733778563724\n",
            "\n",
            "Testing Accuracy: 0.87100\n",
            "Testing Loss: 1.21243\n",
            "Accuracy: 0.87099609375\n",
            "Recall: 0.023848684210526317\n",
            "Precision: 0.17791411042944785\n",
            "F1: 0.04205946337926034\n",
            "AUC: 0.5571878929618238\n",
            "\n",
            "Epoch 3, Loss= 1.308547, Training Accuracy= 0.85156\n",
            "Validation Accuracy: 0.86780\n",
            "Validation Loss: 1.28742\n",
            "Accuracy: 0.8677978515625\n",
            "Recall: 0.026\n",
            "Precision: 0.1925925925925926\n",
            "F1: 0.04581497797356828\n",
            "AUC: 0.5423283103289663\n",
            "\n",
            "Testing Accuracy: 0.87314\n",
            "Testing Loss: 1.19946\n",
            "Accuracy: 0.87314453125\n",
            "Recall: 0.018914473684210526\n",
            "Precision: 0.17829457364341086\n",
            "F1: 0.03420074349442379\n",
            "AUC: 0.5605844577996292\n",
            "\n",
            "Epoch 4, Loss= 1.176494, Training Accuracy= 0.83594\n",
            "Validation Accuracy: 0.85962\n",
            "Validation Loss: 1.23176\n",
            "Accuracy: 0.859619140625\n",
            "Recall: 0.042\n",
            "Precision: 0.1794871794871795\n",
            "F1: 0.06807131280388978\n",
            "AUC: 0.5445108224656136\n",
            "\n",
            "Testing Accuracy: 0.86719\n",
            "Testing Loss: 1.15277\n",
            "Accuracy: 0.8671875\n",
            "Recall: 0.039473684210526314\n",
            "Precision: 0.2\n",
            "F1: 0.06593406593406594\n",
            "AUC: 0.5634295264986159\n",
            "\n",
            "Epoch 5, Loss= 1.135376, Training Accuracy= 0.83594\n",
            "Validation Accuracy: 0.86108\n",
            "Validation Loss: 1.20743\n",
            "Accuracy: 0.861083984375\n",
            "Recall: 0.04\n",
            "Precision: 0.1834862385321101\n",
            "F1: 0.06568144499178982\n",
            "AUC: 0.545096488998144\n",
            "\n",
            "Testing Accuracy: 0.86748\n",
            "Testing Loss: 1.13334\n",
            "Accuracy: 0.86748046875\n",
            "Recall: 0.03700657894736842\n",
            "Precision: 0.19480519480519481\n",
            "F1: 0.06219765031098825\n",
            "AUC: 0.5641543321214643\n",
            "\n",
            "Epoch 6, Loss= 1.110778, Training Accuracy= 0.84375\n",
            "Validation Accuracy: 0.86194\n",
            "Validation Loss: 1.19098\n",
            "Accuracy: 0.8619384765625\n",
            "Recall: 0.036\n",
            "Precision: 0.17733990147783252\n",
            "F1: 0.059850374064837904\n",
            "AUC: 0.5455609899593238\n",
            "\n",
            "Testing Accuracy: 0.86953\n",
            "Testing Loss: 1.11950\n",
            "Accuracy: 0.86953125\n",
            "Recall: 0.033717105263157895\n",
            "Precision: 0.20297029702970298\n",
            "F1: 0.05782792665726375\n",
            "AUC: 0.5658878184634701\n",
            "\n",
            "Epoch 7, Loss= 1.093049, Training Accuracy= 0.84375\n",
            "Validation Accuracy: 0.86609\n",
            "Validation Loss: 1.17769\n",
            "Accuracy: 0.8660888671875\n",
            "Recall: 0.03\n",
            "Precision: 0.1910828025477707\n",
            "F1: 0.05185825410544512\n",
            "AUC: 0.5473113697820539\n",
            "\n",
            "Testing Accuracy: 0.87178\n",
            "Testing Loss: 1.10561\n",
            "Accuracy: 0.87177734375\n",
            "Recall: 0.028782894736842105\n",
            "Precision: 0.20958083832335328\n",
            "F1: 0.05061460592913955\n",
            "AUC: 0.5691721453577013\n",
            "\n",
            "Epoch 8, Loss= 1.088790, Training Accuracy= 0.84766\n",
            "Validation Accuracy: 0.86865\n",
            "Validation Loss: 1.16663\n",
            "Accuracy: 0.86865234375\n",
            "Recall: 0.022\n",
            "Precision: 0.18333333333333332\n",
            "F1: 0.039285714285714285\n",
            "AUC: 0.5479076890440057\n",
            "\n",
            "Testing Accuracy: 0.87461\n",
            "Testing Loss: 1.09521\n",
            "Accuracy: 0.874609375\n",
            "Recall: 0.02138157894736842\n",
            "Precision: 0.21666666666666667\n",
            "F1: 0.038922155688622756\n",
            "AUC: 0.569877426216002\n",
            "\n",
            "Epoch 9, Loss= 1.080292, Training Accuracy= 0.85156\n",
            "Validation Accuracy: 0.87134\n",
            "Validation Loss: 1.15967\n",
            "Accuracy: 0.871337890625\n",
            "Recall: 0.021\n",
            "Precision: 0.21875\n",
            "F1: 0.03832116788321168\n",
            "AUC: 0.5480366349698198\n",
            "\n",
            "Testing Accuracy: 0.87617\n",
            "Testing Loss: 1.08764\n",
            "Accuracy: 0.876171875\n",
            "Recall: 0.01644736842105263\n",
            "Precision: 0.21739130434782608\n",
            "F1: 0.03058103975535168\n",
            "AUC: 0.570949807531209\n",
            "\n",
            "Epoch 10, Loss= 1.070477, Training Accuracy= 0.84766\n",
            "Validation Accuracy: 0.87170\n",
            "Validation Loss: 1.15807\n",
            "Accuracy: 0.8717041015625\n",
            "Recall: 0.014\n",
            "Precision: 0.17721518987341772\n",
            "F1: 0.025949953660797033\n",
            "AUC: 0.549323753067422\n",
            "\n",
            "Testing Accuracy: 0.87676\n",
            "Testing Loss: 1.08657\n",
            "Accuracy: 0.8767578125\n",
            "Recall: 0.013157894736842105\n",
            "Precision: 0.20512820512820512\n",
            "F1: 0.02472952086553323\n",
            "AUC: 0.5722659601206822\n",
            "\n",
            "Epoch 11, Loss= 1.063557, Training Accuracy= 0.85156\n",
            "Validation Accuracy: 0.87244\n",
            "Validation Loss: 1.15206\n",
            "Accuracy: 0.8724365234375\n",
            "Recall: 0.018\n",
            "Precision: 0.2222222222222222\n",
            "F1: 0.03330249768732655\n",
            "AUC: 0.5502544027196026\n",
            "\n",
            "Testing Accuracy: 0.87666\n",
            "Testing Loss: 1.08193\n",
            "Accuracy: 0.87666015625\n",
            "Recall: 0.013157894736842105\n",
            "Precision: 0.20253164556962025\n",
            "F1: 0.02471042471042471\n",
            "AUC: 0.5731463642308515\n",
            "\n",
            "Epoch 12, Loss= 1.048484, Training Accuracy= 0.85156\n",
            "Validation Accuracy: 0.87219\n",
            "Validation Loss: 1.14661\n",
            "Accuracy: 0.8721923828125\n",
            "Recall: 0.019\n",
            "Precision: 0.2235294117647059\n",
            "F1: 0.035023041474654376\n",
            "AUC: 0.5533019361100164\n",
            "\n",
            "Testing Accuracy: 0.87686\n",
            "Testing Loss: 1.07849\n",
            "Accuracy: 0.87685546875\n",
            "Recall: 0.015625\n",
            "Precision: 0.2289156626506024\n",
            "F1: 0.029253271747498075\n",
            "AUC: 0.5748971214893025\n",
            "\n",
            "Epoch 13, Loss= 1.031095, Training Accuracy= 0.85156\n",
            "Validation Accuracy: 0.87170\n",
            "Validation Loss: 1.14253\n",
            "Accuracy: 0.8717041015625\n",
            "Recall: 0.018\n",
            "Precision: 0.20689655172413793\n",
            "F1: 0.03311867525298988\n",
            "AUC: 0.5538652065528138\n",
            "\n",
            "Testing Accuracy: 0.87676\n",
            "Testing Loss: 1.07529\n",
            "Accuracy: 0.8767578125\n",
            "Recall: 0.01644736842105263\n",
            "Precision: 0.23255813953488372\n",
            "F1: 0.030721966205837174\n",
            "AUC: 0.5755362243739043\n",
            "\n",
            "Epoch 14, Loss= 1.020059, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87146\n",
            "Validation Loss: 1.14206\n",
            "Accuracy: 0.8714599609375\n",
            "Recall: 0.017\n",
            "Precision: 0.19540229885057472\n",
            "F1: 0.031278748850046\n",
            "AUC: 0.5567120108146719\n",
            "\n",
            "Testing Accuracy: 0.87725\n",
            "Testing Loss: 1.07520\n",
            "Accuracy: 0.87724609375\n",
            "Recall: 0.018092105263157895\n",
            "Precision: 0.25882352941176473\n",
            "F1: 0.033820138355111454\n",
            "AUC: 0.5775334855341793\n",
            "\n",
            "Epoch 15, Loss= 1.015005, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87207\n",
            "Validation Loss: 1.13946\n",
            "Accuracy: 0.8720703125\n",
            "Recall: 0.019\n",
            "Precision: 0.22093023255813954\n",
            "F1: 0.034990791896869246\n",
            "AUC: 0.5571787630658422\n",
            "\n",
            "Testing Accuracy: 0.87666\n",
            "Testing Loss: 1.07354\n",
            "Accuracy: 0.87666015625\n",
            "Recall: 0.017269736842105265\n",
            "Precision: 0.23595505617977527\n",
            "F1: 0.03218390804597701\n",
            "AUC: 0.5781289424286091\n",
            "\n",
            "Epoch 16, Loss= 1.002660, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87195\n",
            "Validation Loss: 1.13971\n",
            "Accuracy: 0.8719482421875\n",
            "Recall: 0.018\n",
            "Precision: 0.21176470588235294\n",
            "F1: 0.03317972350230415\n",
            "AUC: 0.5588586092483656\n",
            "\n",
            "Testing Accuracy: 0.87725\n",
            "Testing Loss: 1.07345\n",
            "Accuracy: 0.87724609375\n",
            "Recall: 0.017269736842105265\n",
            "Precision: 0.25301204819277107\n",
            "F1: 0.03233256351039261\n",
            "AUC: 0.5797214586459136\n",
            "\n",
            "Epoch 17, Loss= 0.993014, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87207\n",
            "Validation Loss: 1.14182\n",
            "Accuracy: 0.8720703125\n",
            "Recall: 0.02\n",
            "Precision: 0.22727272727272727\n",
            "F1: 0.03676470588235294\n",
            "AUC: 0.5603855913176574\n",
            "\n",
            "Testing Accuracy: 0.87695\n",
            "Testing Loss: 1.07469\n",
            "Accuracy: 0.876953125\n",
            "Recall: 0.015625\n",
            "Precision: 0.23170731707317074\n",
            "F1: 0.029275808936825885\n",
            "AUC: 0.5806824722110184\n",
            "\n",
            "Epoch 18, Loss= 0.984029, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87207\n",
            "Validation Loss: 1.13830\n",
            "Accuracy: 0.8720703125\n",
            "Recall: 0.019\n",
            "Precision: 0.22093023255813954\n",
            "F1: 0.034990791896869246\n",
            "AUC: 0.5609343688294415\n",
            "\n",
            "Testing Accuracy: 0.87734\n",
            "Testing Loss: 1.07158\n",
            "Accuracy: 0.87734375\n",
            "Recall: 0.019736842105263157\n",
            "Precision: 0.2727272727272727\n",
            "F1: 0.03680981595092025\n",
            "AUC: 0.5817406676499907\n",
            "\n",
            "Epoch 19, Loss= 0.975107, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87183\n",
            "Validation Loss: 1.13386\n",
            "Accuracy: 0.871826171875\n",
            "Recall: 0.021\n",
            "Precision: 0.22826086956521738\n",
            "F1: 0.038461538461538464\n",
            "AUC: 0.5624756287224009\n",
            "\n",
            "Testing Accuracy: 0.87695\n",
            "Testing Loss: 1.06835\n",
            "Accuracy: 0.876953125\n",
            "Recall: 0.02055921052631579\n",
            "Precision: 0.26595744680851063\n",
            "F1: 0.03816793893129771\n",
            "AUC: 0.58231561438235\n",
            "\n",
            "Epoch 20, Loss= 0.958842, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87244\n",
            "Validation Loss: 1.13653\n",
            "Accuracy: 0.8724365234375\n",
            "Recall: 0.02\n",
            "Precision: 0.23529411764705882\n",
            "F1: 0.03686635944700461\n",
            "AUC: 0.5631759635789484\n",
            "\n",
            "Testing Accuracy: 0.87686\n",
            "Testing Loss: 1.06949\n",
            "Accuracy: 0.87685546875\n",
            "Recall: 0.019736842105263157\n",
            "Precision: 0.25806451612903225\n",
            "F1: 0.03666921313980138\n",
            "AUC: 0.5833323590168462\n",
            "\n",
            "Epoch 21, Loss= 0.950418, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87183\n",
            "Validation Loss: 1.13086\n",
            "Accuracy: 0.871826171875\n",
            "Recall: 0.022\n",
            "Precision: 0.23404255319148937\n",
            "F1: 0.04021937842778794\n",
            "AUC: 0.5645291122504326\n",
            "\n",
            "Testing Accuracy: 0.87656\n",
            "Testing Loss: 1.06717\n",
            "Accuracy: 0.8765625\n",
            "Recall: 0.023026315789473683\n",
            "Precision: 0.2692307692307692\n",
            "F1: 0.04242424242424243\n",
            "AUC: 0.5833519691861768\n",
            "\n",
            "Epoch 22, Loss= 0.938749, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87219\n",
            "Validation Loss: 1.13100\n",
            "Accuracy: 0.8721923828125\n",
            "Recall: 0.02\n",
            "Precision: 0.22988505747126436\n",
            "F1: 0.03679852805887764\n",
            "AUC: 0.5655758806762503\n",
            "\n",
            "Testing Accuracy: 0.87676\n",
            "Testing Loss: 1.06670\n",
            "Accuracy: 0.8767578125\n",
            "Recall: 0.02138157894736842\n",
            "Precision: 0.2653061224489796\n",
            "F1: 0.0395738203957382\n",
            "AUC: 0.5844388435384633\n",
            "\n",
            "Epoch 23, Loss= 0.931682, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87183\n",
            "Validation Loss: 1.13151\n",
            "Accuracy: 0.871826171875\n",
            "Recall: 0.019\n",
            "Precision: 0.2159090909090909\n",
            "F1: 0.034926470588235295\n",
            "AUC: 0.5663013460000599\n",
            "\n",
            "Testing Accuracy: 0.87715\n",
            "Testing Loss: 1.06684\n",
            "Accuracy: 0.8771484375\n",
            "Recall: 0.023026315789473683\n",
            "Precision: 0.2857142857142857\n",
            "F1: 0.0426179604261796\n",
            "AUC: 0.585265901415162\n",
            "\n",
            "Epoch 24, Loss= 0.924432, Training Accuracy= 0.85156\n",
            "Validation Accuracy: 0.87183\n",
            "Validation Loss: 1.13040\n",
            "Accuracy: 0.871826171875\n",
            "Recall: 0.021\n",
            "Precision: 0.22826086956521738\n",
            "F1: 0.038461538461538464\n",
            "AUC: 0.5674216312802991\n",
            "\n",
            "Testing Accuracy: 0.87666\n",
            "Testing Loss: 1.06530\n",
            "Accuracy: 0.87666015625\n",
            "Recall: 0.022203947368421052\n",
            "Precision: 0.26732673267326734\n",
            "F1: 0.04100227790432802\n",
            "AUC: 0.5864449369362894\n",
            "\n",
            "Epoch 25, Loss= 0.917674, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87219\n",
            "Validation Loss: 1.13154\n",
            "Accuracy: 0.8721923828125\n",
            "Recall: 0.017\n",
            "Precision: 0.20987654320987653\n",
            "F1: 0.03145235892691952\n",
            "AUC: 0.5678434031445013\n",
            "\n",
            "Testing Accuracy: 0.87764\n",
            "Testing Loss: 1.06635\n",
            "Accuracy: 0.87763671875\n",
            "Recall: 0.022203947368421052\n",
            "Precision: 0.2967032967032967\n",
            "F1: 0.041315990818668706\n",
            "AUC: 0.587200927515571\n",
            "\n",
            "Epoch 26, Loss= 0.905335, Training Accuracy= 0.85547\n",
            "Validation Accuracy: 0.87109\n",
            "Validation Loss: 1.12384\n",
            "Accuracy: 0.87109375\n",
            "Recall: 0.022\n",
            "Precision: 0.22\n",
            "F1: 0.04\n",
            "AUC: 0.570256724172991\n",
            "\n",
            "Testing Accuracy: 0.87637\n",
            "Testing Loss: 1.06142\n",
            "Accuracy: 0.8763671875\n",
            "Recall: 0.023848684210526317\n",
            "Precision: 0.26851851851851855\n",
            "F1: 0.04380664652567976\n",
            "AUC: 0.5881921438946943\n",
            "\n",
            "Epoch 27, Loss= 0.896846, Training Accuracy= 0.85156\n",
            "Validation Accuracy: 0.87097\n",
            "Validation Loss: 1.12356\n",
            "Accuracy: 0.8709716796875\n",
            "Recall: 0.024\n",
            "Precision: 0.22857142857142856\n",
            "F1: 0.04343891402714932\n",
            "AUC: 0.5710828286971623\n",
            "\n",
            "Testing Accuracy: 0.87637\n",
            "Testing Loss: 1.06147\n",
            "Accuracy: 0.8763671875\n",
            "Recall: 0.024671052631578948\n",
            "Precision: 0.2727272727272727\n",
            "F1: 0.04524886877828054\n",
            "AUC: 0.5888051614572867\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-77a17d08cbfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m#Run optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyF-KnEFNpP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def generate_batch(data, batch, batch_size, shuffled_indices):\n",
        "    size = batch_size\n",
        "    if size+(batch)*batch_size > len(data):\n",
        "        size = len(data) - batch*batch_size\n",
        "    data_out = []\n",
        "    for i in range(size):\n",
        "        index = shuffled_indices[batch*batch_size+i]\n",
        "        val = data[index]\n",
        "        data_out.append(val)\n",
        "        \n",
        "    return np.array(data_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKT8MwRiLKsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import logistic\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def conf_matrix(preds, truth_list, print_stats = False):\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    tn = 0\n",
        "    fn = 0\n",
        "    \n",
        "    #print(preds[0:10])\n",
        "    #print(\"Sigmoid: \")\n",
        "    #print(logistic.cdf(preds[0:10]))\n",
        "    #print(truth_list.shape)\n",
        "    #print(preds.shape)\n",
        "    \n",
        "    auc = roc_auc_score(truth_list, logistic.cdf(preds))\n",
        "    #print(\"AUC: \" + str(auc))\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "      pred = [1, 0]\n",
        "      if preds[i][1] > preds[i][0]:\n",
        "        pred = [0, 1]\n",
        "      preds[i] = pred\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "      pred = preds[i][1]\n",
        "      truth = truth_list[i][1]\n",
        "      if pred == 1 and truth == 1:\n",
        "        tp += 1\n",
        "      elif pred == 1 and truth == 0:\n",
        "        fp += 1\n",
        "      elif pred == 0 and truth == 0:\n",
        "        tn += 1\n",
        "      elif pred == 0 and truth == 1:\n",
        "        fn += 1\n",
        "\n",
        "    #print(\"True Positive: \" + str(tp))\n",
        "    #print(\"False Positive: \" + str(fp))\n",
        "    #print(\"True Negative: \" + str(tn))\n",
        "    #print(\"False negative: \" + str(fn))\n",
        "\n",
        "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
        "    recall = tp/(tp+fn)\n",
        "    precision = 0\n",
        "    if (tp+fp) > 0:\n",
        "        precision = tp/(tp+fp)\n",
        "    f1 = 2*tp/(2*tp+fp+fn)\n",
        "\n",
        "    if print_stats == True:\n",
        "        print(\"Accuracy: \" + str(accuracy))\n",
        "        print(\"Recall: \" + str(recall))\n",
        "        print(\"Precision: \" + str(precision))\n",
        "        print(\"F1: \" + str(f1))\n",
        "        print(\"\")\n",
        "    \n",
        "    return tp, fp, tn, fn, auc"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}